{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281eddf9-2e56-45cd-93b8-712e1bec6075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model from Webcam Verification: mobilenetv2\n",
      "Model mobilenetv2 loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 826ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001CF8884FB00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001CF8D0E5080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 986ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806ms/step\n",
      "Selected Model from Webcam Verification: ResNet50\n",
      "Model ResNet50 loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Selected Model from Webcam Verification: mobilenetv2\n",
      "Model mobilenetv2 loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 792ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811ms/step\n",
      "Selected Model from Webcam Verification: ResNet50\n",
      "Model ResNet50 loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Selected Model from Webcam Verification: mobilenetv2\n",
      "Model mobilenetv2 loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815ms/step\n",
      "Selected Model from Image Verification: mobilenetv2\n",
      "Model mobilenetv2 loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 831ms/step\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadee\\anaconda3\\envs\\session2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "from PyQt5.QtWidgets import QApplication, QFileDialog\n",
    "import numpy as np\n",
    "from functions import get_image_embeddings, preprocess_to_lfw  # Assuming you have these helper functions\n",
    "\n",
    "from PyQt5.QtCore import QTimer\n",
    "\n",
    "\n",
    "class FaceRecognitionGUI(QtWidgets.QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # other initializations...\n",
    "        self.webcam_timer = QTimer(self)\n",
    "        self.webcam_timer.timeout.connect(self.update_webcam)  # Connect the timer to the update function\n",
    "\n",
    "        self.setWindowTitle(\"Face Recognition\")\n",
    "        self.setGeometry(100, 100, 900, 666)\n",
    "\n",
    "        # Create central widget and set layout\n",
    "        self.centralwidget = QtWidgets.QWidget(self)\n",
    "        self.setCentralWidget(self.centralwidget)\n",
    "\n",
    "        self.image_verification = QtWidgets.QTabWidget(self.centralwidget)\n",
    "        self.image_verification.setGeometry(QtCore.QRect(10, 10, 881, 611))\n",
    "\n",
    "        # Image Verification Tab (First Tab)\n",
    "        self.widget = QtWidgets.QWidget(self)\n",
    "        self.image_verification.addTab(self.widget, \"Image Verification\")\n",
    "\n",
    "        # Initialize Models combo box for the first tab\n",
    "        self.Models = QtWidgets.QComboBox(self.widget)\n",
    "        self.Models.setGeometry(QtCore.QRect(310, 30, 231, 22))\n",
    "        self.Models.setFont(QtGui.QFont('Arial', 10, QtGui.QFont.Bold))\n",
    "        self.Models.addItem(\"ResNet50\")\n",
    "        self.Models.addItem(\"mobilenetv2\")\n",
    "\n",
    "        self.label = QtWidgets.QLabel(\"Model Selection\", self.widget)\n",
    "        self.label.setGeometry(QtCore.QRect(310, 10, 221, 20))\n",
    "        self.label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        # Now connect the signal to the slot after initialization\n",
    "        self.Models.currentIndexChanged.connect(self.on_model_change)\n",
    "\n",
    "        # Other widgets for Image Verification tab\n",
    "        self.first_image = QtWidgets.QLabel(self.widget)\n",
    "        self.first_image.setGeometry(QtCore.QRect(30, 170, 361, 351))\n",
    "\n",
    "        self.second_image_label = QtWidgets.QLabel(self.widget)\n",
    "        self.second_image_label.setGeometry(QtCore.QRect(470, 170, 361, 351))\n",
    "\n",
    "        self.upload_first_image = QtWidgets.QPushButton(\"Upload Reference Image\", self.widget)\n",
    "        self.upload_first_image.setGeometry(QtCore.QRect(30, 540, 361, 28))\n",
    "        self.upload_first_image.clicked.connect(self.upload_reference_image)\n",
    "\n",
    "        self.upload_second_image_button = QtWidgets.QPushButton(\"Upload Second Image\", self.widget)\n",
    "        self.upload_second_image_button.setGeometry(QtCore.QRect(470, 540, 361, 28))\n",
    "        self.upload_second_image_button.clicked.connect(self.upload_second_image)\n",
    "\n",
    "        self.verify = QtWidgets.QPushButton(\"Verify\", self.widget)\n",
    "        self.verify.setGeometry(QtCore.QRect(370, 60, 93, 28))\n",
    "        self.verify.clicked.connect(self.verify_images)\n",
    "\n",
    "        self.results_text = QtWidgets.QTextEdit(self.widget)\n",
    "        self.results_text.setGeometry(QtCore.QRect(193, 90, 441, 41))\n",
    "\n",
    "        # Webcam Verification Tab (Second Tab)\n",
    "        self.tab_2 = QtWidgets.QWidget(self)\n",
    "        self.image_verification.addTab(self.tab_2, \"Webcam Verification\")\n",
    "\n",
    "        self.ref_image_label_2 = QtWidgets.QLabel(self.tab_2)\n",
    "        self.ref_image_label_2.setGeometry(QtCore.QRect(30, 170, 361, 351))\n",
    "\n",
    "        self.webcam_label = QtWidgets.QLabel(self.tab_2)\n",
    "        self.webcam_label.setGeometry(QtCore.QRect(470, 170, 361, 351))\n",
    "\n",
    "        self.upload_ref_image_btn_2 = QtWidgets.QPushButton(\"Upload Reference Image\", self.tab_2)\n",
    "        self.upload_ref_image_btn_2.setGeometry(QtCore.QRect(30, 540, 361, 28))\n",
    "        self.upload_ref_image_btn_2.clicked.connect(self.upload_reference_image_2)\n",
    "\n",
    "        # Initialize Models combo box for the second tab\n",
    "        self.Models_2 = QtWidgets.QComboBox(self.tab_2)\n",
    "        self.Models_2.setGeometry(QtCore.QRect(310, 30, 231, 22))\n",
    "        self.Models_2.setFont(QtGui.QFont('Arial', 10, QtGui.QFont.Bold))\n",
    "        self.Models_2.addItem(\"ResNet50\")\n",
    "        self.Models_2.addItem(\"mobilenetv2\")\n",
    "\n",
    "        self.webcam_btn = QtWidgets.QPushButton(\"Start Webcam\", self.tab_2)\n",
    "        self.webcam_btn.setGeometry(QtCore.QRect(470, 540, 180, 28))\n",
    "        self.webcam_btn.clicked.connect(self.toggle_webcam)\n",
    "\n",
    "        self.verify_webcam_btn = QtWidgets.QPushButton(\"Verify Webcam\", self.tab_2)\n",
    "        self.verify_webcam_btn.setGeometry(QtCore.QRect(651, 540, 180, 28))\n",
    "        self.verify_webcam_btn.clicked.connect(self.verify_webcam)\n",
    "\n",
    "        self.webcam_results_text = QtWidgets.QTextEdit(self.tab_2)\n",
    "        self.webcam_results_text.setGeometry(QtCore.QRect(193, 90, 441, 41))\n",
    "\n",
    "        # Now connect the signal to the slot for the second combo box\n",
    "        self.Models_2.currentIndexChanged.connect(self.on_model_change)\n",
    "\n",
    "        # Initialize webcam capture\n",
    "        self.webcam = None\n",
    "        self.current_model = None\n",
    "        self.threshold = 0.3\n",
    "        self.model_paths = {\n",
    "            \"ResNet50\": \"resnet50.keras\",\n",
    "            \"mobilenetv2\": \"mobilenetv2.keras\",\n",
    "        }\n",
    "        self.cached_image1_path = None\n",
    "        self.cached_image2_path = None\n",
    "        self.cached_encoding1 = None\n",
    "        self.cached_encoding2 = None\n",
    "        self.ref_image_path = None\n",
    "\n",
    "    # Remaining methods (load_model, upload_reference_image, etc.) should remain the same\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name in self.model_paths:\n",
    "            self.current_model = tf.keras.models.load_model(self.model_paths[model_name])\n",
    "            print(f\"Model {model_name} loaded successfully.\")\n",
    "    \n",
    "            # Clear cached data to avoid conflicts when switching models\n",
    "            self.cached_image1_path = None\n",
    "            self.cached_image2_path = None\n",
    "            self.cached_encoding1 = None\n",
    "            self.cached_encoding2 = None\n",
    "            self.ref_image_path = None\n",
    "    def upload_reference_image(self):\n",
    "        file_name, _ = QFileDialog.getOpenFileName(\n",
    "            self, \"Open Reference Image\", \"\", \"Image Files (*.png *.jpg *.jpeg)\"\n",
    "        )\n",
    "        if file_name:\n",
    "            image = cv2.imread(file_name)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = sorted(faces, key=lambda face: face[2] * face[3], reverse=True)[0]\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            height, width, channel = image.shape\n",
    "            bytes_per_line = 3 * width\n",
    "            q_image = QImage(image.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "            pixmap = QPixmap.fromImage(q_image)\n",
    "            \n",
    "            self.ref_image_path = file_name\n",
    "            self.first_image.setPixmap(pixmap.scaled(400, 400, QtCore.Qt.KeepAspectRatio))\n",
    "    def on_model_change(self):\n",
    "        # Check which combo box is calling this function\n",
    "        if self.sender() == self.Models:  # For Image Verification tab\n",
    "            selected_model = self.Models.currentText()\n",
    "            print(f\"Selected Model from Image Verification: {selected_model}\")\n",
    "            self.load_model(selected_model)\n",
    "        elif self.sender() == self.Models_2:  # For Webcam Verification tab\n",
    "            selected_model = self.Models_2.currentText()\n",
    "            print(f\"Selected Model from Webcam Verification: {selected_model}\")\n",
    "            self.load_model(selected_model)\n",
    "\n",
    "    def upload_reference_image_2(self):\n",
    "        file_name, _ = QFileDialog.getOpenFileName(\n",
    "            self, \"Open Reference Image\", \"\", \"Image Files (*.png *.jpg *.jpeg)\"\n",
    "        )\n",
    "        if file_name:\n",
    "            image = cv2.imread(file_name)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = sorted(faces, key=lambda face: face[2] * face[3], reverse=True)[0]\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            height, width, channel = image.shape\n",
    "            bytes_per_line = 3 * width\n",
    "            q_image = QImage(image.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "            pixmap = QPixmap.fromImage(q_image)\n",
    "            \n",
    "            self.ref_image_path = file_name\n",
    "            self.ref_image_label_2.setPixmap(pixmap.scaled(400, 400, QtCore.Qt.KeepAspectRatio))\n",
    "    def upload_second_image(self):\n",
    "        options = QFileDialog.Options()\n",
    "        file, _ = QFileDialog.getOpenFileName(self, \"Upload Second Image\", \"\", \"Images (*.png *.xpm *.jpg *.jpeg);;All Files (*)\", options=options)\n",
    "        \n",
    "        if file:\n",
    "            # Store the second image path\n",
    "            self.second_image_path = file\n",
    "            self.second_image_label.setPixmap(QtGui.QPixmap(file).scaled(self.second_image_label.size(), QtCore.Qt.KeepAspectRatio))\n",
    "    \n",
    "            # Use the correct 'file' variable, not 'file_name'\n",
    "            image = cv2.imread(file)  # Corrected variable name\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "            if len(faces) > 0:\n",
    "                # Sort faces by the largest area and draw a rectangle around the largest one\n",
    "                x, y, w, h = sorted(faces, key=lambda face: face[2] * face[3], reverse=True)[0]\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "            # Convert the image to RGB format for display\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            height, width, channel = image.shape\n",
    "            bytes_per_line = 3 * width\n",
    "            q_image = QImage(image.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "            pixmap = QPixmap.fromImage(q_image)\n",
    "    \n",
    "            # Update the label with the processed image\n",
    "            self.second_image_label.setPixmap(pixmap.scaled(400, 400, QtCore.Qt.KeepAspectRatio))\n",
    "    \n",
    "\n",
    "    def verify_images(self):\n",
    "        # Check if both images are uploaded\n",
    "        if not hasattr(self, 'ref_image_path') or self.ref_image_path is None:\n",
    "            self.results_text.setText(\"Please upload a reference image before verifying.\")\n",
    "            return\n",
    "    \n",
    "        if not hasattr(self, 'second_image_path') or self.second_image_path is None:\n",
    "            self.results_text.setText(\"Please upload a second image before verifying.\")\n",
    "            return\n",
    "    \n",
    "        # Ensure the model is loaded\n",
    "        if self.current_model is None:\n",
    "            self.load_model(self.Models.currentText())\n",
    "    \n",
    "        # Load and process images\n",
    "        image1 = cv2.imread(self.ref_image_path)\n",
    "        image2 = cv2.imread(self.second_image_path)\n",
    "    \n",
    "        # Check if embeddings are already cached for the first image\n",
    "        if not hasattr(self, 'cached_image1_path') or self.cached_image1_path != self.ref_image_path:\n",
    "            try:\n",
    "                self.cached_encoding1 = get_image_embeddings(self.current_model, image1, False)\n",
    "                self.cached_image1_path = self.ref_image_path\n",
    "            except ValueError as e:\n",
    "                if str(e) == \"No faces detected in the image.\":\n",
    "                    self.results_text.setText(\"No face detected in the first image.\")\n",
    "                    return\n",
    "                else:\n",
    "                    self.results_text.setText(\"An error occurred during image verification.\")\n",
    "                    return\n",
    "    \n",
    "        # Check if embeddings are already cached for the second image\n",
    "        if not hasattr(self, 'cached_image2_path') or self.cached_image2_path != self.second_image_path:\n",
    "            try:\n",
    "                self.cached_encoding2 = get_image_embeddings(self.current_model, image2, False)\n",
    "                self.cached_image2_path = self.second_image_path\n",
    "            except ValueError as e:\n",
    "                if str(e) == \"No faces detected in the image.\":\n",
    "                    self.results_text.setText(\"No face detected in the second image.\")\n",
    "                    return\n",
    "                else:\n",
    "                    self.results_text.setText(\"An error occurred during image verification.\")\n",
    "                    return\n",
    "    \n",
    "        # Calculate similarity metrics\n",
    "        distance = tf.norm(self.cached_encoding1 - self.cached_encoding2).numpy()\n",
    "        cosine_similarity = tf.tensordot(self.cached_encoding1, self.cached_encoding2, axes=1).numpy() / (\n",
    "            tf.norm(self.cached_encoding1).numpy() * tf.norm(self.cached_encoding2).numpy()\n",
    "        )\n",
    "    \n",
    "        # Display the results\n",
    "\n",
    "        results = f\"{'Match:':<20}{'Yes' if cosine_similarity > self.threshold else 'No'}\"\n",
    "        self.results_text.setText(results)\n",
    "\n",
    "    def toggle_webcam(self):\n",
    "        if self.webcam is None:\n",
    "            self.webcam = cv2.VideoCapture(0)\n",
    "            self.webcam_btn.setText(\"Stop Webcam\")\n",
    "            self.webcam_timer.start(30)\n",
    "        else:\n",
    "            self.webcam_timer.stop()\n",
    "            self.webcam.release()\n",
    "            self.webcam = None\n",
    "            self.webcam_btn.setText(\"Start Webcam\")\n",
    "            self.webcam_label.clear()\n",
    "\n",
    "    def update_webcam(self):\n",
    "        ret, frame = self.webcam.read()\n",
    "        if ret:\n",
    "            self.current_frame = frame.copy()  # Copy for processing purposes\n",
    "            display_frame = frame.copy()  # Copy for display purposes\n",
    "            gray = cv2.cvtColor(display_frame, cv2.COLOR_BGR2GRAY)\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = sorted(faces, key=lambda face: face[2] * face[3], reverse=True)[0]\n",
    "                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            display_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)\n",
    "            height, width, channel = display_frame.shape\n",
    "            bytes_per_line = 3 * width\n",
    "            q_image = QImage(display_frame.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "            pixmap = QPixmap.fromImage(q_image)\n",
    "            self.webcam_label.setPixmap(pixmap.scaled(400, 400, QtCore.Qt.KeepAspectRatio))\n",
    "    def upload_reference_image_2_function(self):\n",
    "        file_name, _ = QFileDialog.getOpenFileName(\n",
    "            self, \"Open Reference Image\", \"\", \"Image Files (*.png *.jpg *.jpeg)\"\n",
    "        )\n",
    "        if file_name:\n",
    "            # Load the image\n",
    "            image = cv2.imread(file_name)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "            # Load OpenCV's pre-trained face detector\n",
    "            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "            # Detect faces\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "            if len(faces) > 0:\n",
    "                # Sort faces by size (area) and get the largest one\n",
    "                x, y, w, h = sorted(faces, key=lambda face: face[2] * face[3], reverse=True)[0]\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "            # Convert the image to QPixmap for display\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            height, width, channel = image.shape\n",
    "            bytes_per_line = 3 * width\n",
    "            q_image = QImage(image.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "            pixmap = QPixmap.fromImage(q_image)\n",
    "    \n",
    "            self.first_image_2.setPixmap(pixmap.scaled(400, 400, QtCore.Qt.KeepAspectRatio))\n",
    "            self.ref_image_path = file_name  # Store the path of the reference image\n",
    "\n",
    "    def verify_webcam(self):\n",
    "        # Ensure that the required attributes are present\n",
    "        if not hasattr(self, 'ref_image_path') or not hasattr(self, 'current_frame'):\n",
    "            self.webcam_results_text.setText(\"Reference image or webcam frame not available!\")\n",
    "            return\n",
    "    \n",
    "        # Load the model if it hasn't been loaded yet\n",
    "        if self.current_model is None:\n",
    "            self.load_model(self.Models.currentText())\n",
    "    \n",
    "        # Check if reference image is valid\n",
    "        ref_image = cv2.imread(self.ref_image_path)\n",
    "        if ref_image is None:\n",
    "            self.webcam_results_text.setText(\"Failed to load reference image!\")\n",
    "            return\n",
    "    \n",
    "        # Check if current frame from webcam is valid\n",
    "        if self.current_frame is None:\n",
    "            self.webcam_results_text.setText(\"No frame captured from webcam!\")\n",
    "            return\n",
    "    \n",
    "        try:\n",
    "            # Get embeddings for the reference image and webcam frame\n",
    "            ref_encoding = get_image_embeddings(self.current_model, ref_image, False)\n",
    "            webcam_encoding = get_image_embeddings(self.current_model, self.current_frame, False)\n",
    "    \n",
    "            # Compute distance (Euclidean distance)\n",
    "            distance = tf.norm(ref_encoding - webcam_encoding).numpy()\n",
    "    \n",
    "            # Compute cosine similarity\n",
    "            cosine_similarity = tf.tensordot(ref_encoding, webcam_encoding, axes=1).numpy() / (\n",
    "                tf.norm(ref_encoding).numpy() * tf.norm(webcam_encoding).numpy()\n",
    "            )\n",
    "    \n",
    "            # Format the results\n",
    "            results = f\"{'Match:':<20}{'Yes' if cosine_similarity > self.threshold else 'No'}\"\n",
    "    \n",
    "            # Update the text box with the results\n",
    "            self.webcam_results_text.setText(results)\n",
    "    \n",
    "        except ValueError as e:\n",
    "            # Handle specific error when no face is detected\n",
    "            if str(e) == \"No faces detected in the image.\":\n",
    "                self.webcam_results_text.setText(\"No face detected in the reference or webcam image.\")\n",
    "            else:\n",
    "                self.webcam_results_text.setText(f\"An error occurred during image verification: {str(e)}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            # General exception handling\n",
    "            self.webcam_results_text.setText(f\"Unexpected error: {str(e)}\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        if self.webcam is not None:\n",
    "            self.webcam.release()\n",
    "        event.accept()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QApplication(sys.argv)\n",
    "    window = FaceRecognitionGUI()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac6821-1813-4116-acd4-10705356f622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
